{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "#sys.path.append('/home/biaslab/Zhen/HDC_DINO')\n",
    "import torch, json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from main import build_model_main\n",
    "from util.slconfig import SLConfig\n",
    "from datasets import build_dataset\n",
    "from util.visualizer import COCOVisualizer\n",
    "from util import box_ops\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_path = \"/home/biaslab/Zhen/HDC_DINO/config/DINO/DINO_4scale.py\" # change the path of the model config file\n",
    "model_checkpoint_path = \"/home/biaslab/Zhen/HDC_DINO/checkpoint/checkpoint0033_4scale.pth\"  # change the path of the model checkpoint\n",
    "# See our Model Zoo section in README.md for more details about our pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biaslab/miniconda3/envs/hdc_dino/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/biaslab/miniconda3/envs/hdc_dino/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/biaslab/miniconda3/envs/hdc_dino/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/biaslab/miniconda3/envs/hdc_dino/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_2214758/2906384000.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_checkpoint_path, map_location=args.device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = SLConfig.fromfile(model_config_path) \n",
    "args.device = 'cuda' \n",
    "model, criterion, postprocessors = build_model_main(args)\n",
    "checkpoint = torch.load(model_checkpoint_path, map_location=args.device)\n",
    "state_dict = checkpoint['model']\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "# # Filter out keys for class_embed\n",
    "# filtered_state_dict = {k: v for k, v in state_dict.items() if \"class_embed\" not in k}\n",
    "# model.load_state_dict(filtered_state_dict, strict=False)\n",
    "# custom_class_embed = torch.load(\"/home/biaslab/Zhen/DINO/Demo/mlp_ckpt/frcnn_mlp_0.pth\", map_location=args.device)\n",
    "# model.class_embed.load_state_dict(custom_class_embed)\n",
    "# # Print out the state dictionary for the classification head to verify custom weights\n",
    "# print(\"Custom classification head state dict keys:\")\n",
    "# for key, value in model.class_embed.state_dict().items():\n",
    "#     print(f\"{key}: norm = {value.norm().item():.4f}\")\n",
    "# _ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load coco names\n",
    "with open('/home/biaslab/Zhen/HDC_DINO/util/coco_id2name.json') as f:\n",
    "    id2name = json.load(f)\n",
    "    id2name = {int(k):v for k,v in id2name.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "from pycocotools import mask as maskUtils\n",
    "from matplotlib import transforms\n",
    "\n",
    "def addtgt_cv(frame, tgt):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes and annotations directly on an OpenCV image.\n",
    "    \n",
    "    Args:\n",
    "        frame (np.ndarray): The image/frame on which to draw (BGR format).\n",
    "        tgt (dict): A dictionary containing:\n",
    "            - 'boxes': Tensor of shape [num_boxes, 4] in normalized xywh format (center, width, height).\n",
    "            - 'size': Tensor/list with [H, W] of the canonical image.\n",
    "            - 'box_label': (optional) list of labels for each box.\n",
    "            - 'caption': (optional) a caption string.\n",
    "    Returns:\n",
    "        np.ndarray: The annotated frame.\n",
    "    \"\"\"\n",
    "    # Get canonical target dimensions and actual frame dimensions\n",
    "    target_h, target_w = tgt['size'].tolist()  # 401, 1331 from your tgt\n",
    "    frame_h, frame_w, _ = frame.shape         # 374, 1242 from your frame\n",
    "\n",
    "    # Compute scaling factors from target size to frame size\n",
    "    scale_x = frame_w / target_w\n",
    "    scale_y = frame_h / target_h\n",
    "\n",
    "    boxes = []\n",
    "    colors = []\n",
    "\n",
    "    # Process each bounding box\n",
    "    for box in tgt['boxes'].cpu():\n",
    "        # Convert normalized center-based xywh to top-left based coordinates\n",
    "        # First, compute the box relative to the canonical target size\n",
    "        unnormbbox = box * torch.tensor([target_w, target_h, target_w, target_h])\n",
    "        unnormbbox[:2] -= unnormbbox[2:] / 2  # shift from center to top-left\n",
    "        \n",
    "        # Rescale coordinates to match the actual frame dimensions\n",
    "        unnormbbox[0] *= scale_x  # x coordinate\n",
    "        unnormbbox[1] *= scale_y  # y coordinate\n",
    "        unnormbbox[2] *= scale_x  # width\n",
    "        unnormbbox[3] *= scale_y  # height\n",
    "        \n",
    "        bbox = unnormbbox.round().int().tolist()  # round to nearest int\n",
    "        boxes.append(bbox)\n",
    "        \n",
    "        # Generate a random color (BGR) for visualization\n",
    "        c = (np.random.rand(3) * 0.6 + 0.4) * 255  # roughly values in [102,255]\n",
    "        c = tuple(int(x) for x in c)\n",
    "        colors.append(c)\n",
    "    \n",
    "    # Create an overlay to draw filled rectangles with transparency\n",
    "    overlay = frame.copy()\n",
    "    \n",
    "    # Draw each rectangle and its label\n",
    "    for i, bbox in enumerate(boxes):\n",
    "        x, y, w, h = bbox  # already integers\n",
    "        # Draw a filled rectangle on the overlay\n",
    "        cv2.rectangle(overlay, (x, y), (x + w, y + h), colors[i], -1)\n",
    "        # Draw the rectangle border on the original frame\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), colors[i], 2)\n",
    "        # If label is provided, add text above the rectangle\n",
    "        if 'box_label' in tgt:\n",
    "            label = str(tgt['box_label'][i])\n",
    "            cv2.putText(frame, label, (x, y - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[i], 1, cv2.LINE_AA)\n",
    "    \n",
    "    # Blend the overlay with the frame to get transparent filled boxes\n",
    "    alpha = 0.1  # transparency factor\n",
    "    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "    \n",
    "    # If a caption is provided, add it to the top of the frame\n",
    "    if 'caption' in tgt:\n",
    "        caption = tgt['caption']\n",
    "        cv2.putText(frame, caption, (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "\n",
    "def addtgt(tgt):\n",
    "        \"\"\"\n",
    "        - tgt: dict. args:\n",
    "            - boxes: num_boxes, 4. xywh, [0,1].\n",
    "            - box_label: num_boxes.\n",
    "        \"\"\"\n",
    "        assert 'boxes' in tgt\n",
    "        ax = plt.gca()\n",
    "        H, W = tgt['size'].tolist() \n",
    "        numbox = tgt['boxes'].shape[0]\n",
    "\n",
    "        color = []\n",
    "        polygons = []\n",
    "        boxes = []\n",
    "        for box in tgt['boxes'].cpu():\n",
    "            unnormbbox = box * torch.Tensor([W, H, W, H])\n",
    "            unnormbbox[:2] -= unnormbbox[2:] / 2\n",
    "            [bbox_x, bbox_y, bbox_w, bbox_h] = unnormbbox.tolist()\n",
    "            boxes.append([bbox_x, bbox_y, bbox_w, bbox_h])\n",
    "            poly = [[bbox_x, bbox_y], [bbox_x, bbox_y+bbox_h], [bbox_x+bbox_w, bbox_y+bbox_h], [bbox_x+bbox_w, bbox_y]]\n",
    "            np_poly = np.array(poly).reshape((4,2))\n",
    "            polygons.append(Polygon(np_poly))\n",
    "            c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n",
    "            color.append(c)\n",
    "\n",
    "        p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.1)\n",
    "        ax.add_collection(p)\n",
    "        p = PatchCollection(polygons, facecolor='none', edgecolors=color, linewidths=2)\n",
    "        ax.add_collection(p)\n",
    "\n",
    "\n",
    "        if 'box_label' in tgt:\n",
    "            assert len(tgt['box_label']) == numbox, f\"{len(tgt['box_label'])} = {numbox}, \"\n",
    "            for idx, bl in enumerate(tgt['box_label']):\n",
    "                _string = str(bl)\n",
    "                bbox_x, bbox_y, bbox_w, bbox_h = boxes[idx]\n",
    "                # ax.text(bbox_x, bbox_y, _string, color='black', bbox={'facecolor': 'yellow', 'alpha': 1.0, 'pad': 1})\n",
    "                ax.text(bbox_x, bbox_y, _string, color='black', bbox={'facecolor': color[idx], 'alpha': 0.6, 'pad': 1})\n",
    "\n",
    "        if 'caption' in tgt:\n",
    "            ax.set_title(tgt['caption'], wrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.visualizer import COCOVisualizer, renorm\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import datasets.transforms as T\n",
    "import torchvision.transforms as TF\n",
    "# transform images\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def vis_frame(frame, model, savedir = '/home/biaslab/Zhen/HDC_DINO/Demo/processed_frame', thershold = 0.3, caption=None):\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    image = TF.Resize((368, 600))(image)\n",
    "    image, _ = transform(image, None)\n",
    "    with torch.no_grad():\n",
    "        output = model.cuda()(image[None].cuda())\n",
    "        output = postprocessors['bbox'](output, torch.Tensor([[1.0, 1.0]]).cuda())[0]\n",
    "\n",
    "\n",
    "    vslzr = COCOVisualizer()\n",
    "\n",
    "    scores = output['scores']\n",
    "    labels = output['labels']\n",
    "    boxes = box_ops.box_xyxy_to_cxcywh(output['boxes'])\n",
    "    #print(scores[:10])\n",
    "    score_to_name = {score: id2name[int(label)] for score, label in zip(scores, labels)}\n",
    "    top25 = sorted(score_to_name.items(), key=lambda x: x[0].item(), reverse=True)[:25]\n",
    "    with open('top25.txt', 'w') as f:\n",
    "        for score, label in top25:\n",
    "            f.write(f\"{score.item()}: {label}\\n\")\n",
    "    print(top25)\n",
    "    select_mask = scores > thershold\n",
    "\n",
    "    box_label = [id2name[int(item)] for item in labels[select_mask]]\n",
    "    pred_dict = {\n",
    "        'boxes': boxes[select_mask],\n",
    "        'size': torch.Tensor([image.shape[1], image.shape[2]]),\n",
    "        'box_label': box_label\n",
    "    }\n",
    "    # plt.figure(dpi=120)\n",
    "    # plt.rcParams['font.size'] = '5'\n",
    "    # ax = plt.gca()\n",
    "    # image = renorm(image).permute(1, 2, 0)\n",
    "    #ax.imshow(image)\n",
    "    processed_frame = addtgt_cv(frame, pred_dict)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    #processed_frame = capture_figure_as_frame()\n",
    "    #processed_frame = capture_figure_as_frame_buffer()\n",
    "    #plt.show()\n",
    "    #plt.close()\n",
    "    #return processed_frame\n",
    "    #vslzr.visualize(image, pred_dict, savedir=None, dpi=100)\n",
    "    return processed_frame \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    #  Load pre-trained Faster R-CNN model \n",
    "    # model = fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "    # model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # # state_dict = torch.load('ckpt/linear_9.pth')\n",
    "    # # model.roi_heads.box_predictor.cls_score.weight = torch.nn.Parameter(state_dict['fc.weight'])\n",
    "    # # model.roi_heads.box_predictor.cls_score.bias = torch.nn.Parameter(state_dict['fc.bias'])\n",
    "    args = SLConfig.fromfile(model_config_path) \n",
    "    args.device = 'cuda' \n",
    "    model, criterion, postprocessors = build_model_main(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=args.device)\n",
    "    state_dict = checkpoint['model']\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    # Filter out keys for class_embed\n",
    "    filtered_state_dict = {k: v for k, v in state_dict.items() if \"class_embed\" not in k}\n",
    "    model.load_state_dict(filtered_state_dict, strict=False)\n",
    "    custom_class_embed = torch.load(\"/home/biaslab/Zhen/HDC_DINO/Demo/hd_ckpt_100/hd_classification_head_epoch_11.pth\", map_location=args.device)\n",
    "    custom_bbox_embed = torch.load(\"/home/biaslab/Zhen/HDC_DINO/Demo/hd_ckpt_100/regression_head_epoch_11.pth\", map_location=args.device)\n",
    "    model.class_embed.load_state_dict(custom_class_embed)\n",
    "    model.bbox_embed.load_state_dict(custom_bbox_embed)\n",
    "    _ = model.eval()\n",
    "    #print(model)\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture('/home/biaslab/Zhen/HDC_DINO/videos/0001.mp4')\n",
    "\n",
    "    # Video writer to save output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter('/home/biaslab/Zhen/HDC_DINO/output_hd_100.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS),\n",
    "                          (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "    # Process video frame by frame\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        #vis_frame(frame, model,  thershold = 0.3)\n",
    "        processed_frame = vis_frame(frame, model, thershold = 0.3)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        out.write(processed_frame)\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Processed video saved at /home/biaslab/Zhen/HDC_DINO/output_hd_100.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2214758/3579479952.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_checkpoint_path, map_location=args.device)\n",
      "/tmp/ipykernel_2214758/3579479952.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  custom_class_embed = torch.load(\"/home/biaslab/Zhen/HDC_DINO/Demo/hd_ckpt_100/hd_classification_head_epoch_11.pth\", map_location=args.device)\n",
      "/tmp/ipykernel_2214758/3579479952.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  custom_bbox_embed = torch.load(\"/home/biaslab/Zhen/HDC_DINO/Demo/hd_ckpt_100/regression_head_epoch_11.pth\", map_location=args.device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ModuleList:\n\tMissing key(s) in state_dict: \"0.weight\", \"0.bias\", \"1.weight\", \"1.bias\", \"2.weight\", \"2.bias\", \"3.weight\", \"3.bias\", \"4.weight\", \"4.bias\", \"5.weight\", \"5.bias\". \n\tUnexpected key(s) in state_dict: \"0.encoder.weight\", \"0.encoder.bias\", \"0.model.weight\", \"0.model.bias\", \"1.encoder.weight\", \"1.encoder.bias\", \"1.model.weight\", \"1.model.bias\", \"2.encoder.weight\", \"2.encoder.bias\", \"2.model.weight\", \"2.model.bias\", \"3.encoder.weight\", \"3.encoder.bias\", \"3.model.weight\", \"3.model.bias\", \"4.encoder.weight\", \"4.encoder.bias\", \"4.model.weight\", \"4.model.bias\", \"5.encoder.weight\", \"5.encoder.bias\", \"5.model.weight\", \"5.model.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m custom_class_embed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/biaslab/Zhen/HDC_DINO/Demo/hd_ckpt_100/hd_classification_head_epoch_11.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     23\u001b[0m custom_bbox_embed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/biaslab/Zhen/HDC_DINO/Demo/hd_ckpt_100/regression_head_epoch_11.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_embed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_class_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mbbox_embed\u001b[38;5;241m.\u001b[39mload_state_dict(custom_bbox_embed)\n\u001b[1;32m     26\u001b[0m _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/hdc_dino/lib/python3.8/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ModuleList:\n\tMissing key(s) in state_dict: \"0.weight\", \"0.bias\", \"1.weight\", \"1.bias\", \"2.weight\", \"2.bias\", \"3.weight\", \"3.bias\", \"4.weight\", \"4.bias\", \"5.weight\", \"5.bias\". \n\tUnexpected key(s) in state_dict: \"0.encoder.weight\", \"0.encoder.bias\", \"0.model.weight\", \"0.model.bias\", \"1.encoder.weight\", \"1.encoder.bias\", \"1.model.weight\", \"1.model.bias\", \"2.encoder.weight\", \"2.encoder.bias\", \"2.model.weight\", \"2.model.bias\", \"3.encoder.weight\", \"3.encoder.bias\", \"3.model.weight\", \"3.model.bias\", \"4.encoder.weight\", \"4.encoder.bias\", \"4.model.weight\", \"4.model.bias\", \"5.encoder.weight\", \"5.encoder.bias\", \"5.model.weight\", \"5.model.bias\". "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
